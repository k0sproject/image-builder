From 69d2b5bb5e6b41a3c6950ff03a25ddfc7c38e7d7 Mon Sep 17 00:00:00 2001
From: Jiawei Huang <jiawei@tigera.io>
Date: Wed, 11 Oct 2023 10:55:02 -0700
Subject: [PATCH 19/22] Fix kube-controller deprecated function
 wait.PollImmediate

---
 apiserver/test/integration/framework.go       |  5 +-
 apiserver/test/util/util.go                   | 10 ++--
 .../flannelmigration/k8s_resources.go         | 36 ++++++-------
 libcalico-go/lib/backend/k8s/k8s_test.go      |  4 +-
 .../upgrade/migrator/clients/v1/k8s/k8s.go    | 54 +------------------
 5 files changed, 29 insertions(+), 80 deletions(-)

diff --git a/apiserver/test/integration/framework.go b/apiserver/test/integration/framework.go
index e3f333b7e..697f8177c 100644
--- a/apiserver/test/integration/framework.go
+++ b/apiserver/test/integration/framework.go
@@ -15,6 +15,7 @@
 package integration
 
 import (
+	"context"
 	"crypto/tls"
 	"fmt"
 	"math/rand"
@@ -155,8 +156,8 @@ func waitForApiserverUp(serverURL string, stopCh <-chan struct{}) error {
 	timeout := 30 * time.Second
 	startWaiting := time.Now()
 	tries := 0
-	return wait.PollImmediate(interval, timeout,
-		func() (bool, error) {
+	return wait.PollUntilContextTimeout(context.Background(), interval, timeout, true,
+		func(ctx context.Context) (bool, error) {
 			select {
 			// we've been told to stop, so no reason to keep going
 			case <-stopCh:
diff --git a/apiserver/test/util/util.go b/apiserver/test/util/util.go
index 90e090651..95446a98c 100644
--- a/apiserver/test/util/util.go
+++ b/apiserver/test/util/util.go
@@ -30,10 +30,10 @@ import (
 // WaitForGlobalNetworkPoliciesToNotExist waits for the GlobalNetworkPolicy with the given name to no
 // longer exist.
 func WaitForGlobalNetworkPoliciesToNotExist(client calicoclient.ProjectcalicoV3Interface, name string) error {
-	return wait.PollImmediate(500*time.Millisecond, wait.ForeverTestTimeout,
-		func() (bool, error) {
+	return wait.PollUntilContextTimeout(context.Background(), 500*time.Millisecond, wait.ForeverTestTimeout, true,
+		func(ctx context.Context) (bool, error) {
 			klog.V(5).Infof("Waiting for broker %v to not exist", name)
-			_, err := client.GlobalNetworkPolicies().Get(context.Background(), name, metav1.GetOptions{})
+			_, err := client.GlobalNetworkPolicies().Get(ctx, name, metav1.GetOptions{})
 			if nil == err {
 				return false, nil
 			}
@@ -50,8 +50,8 @@ func WaitForGlobalNetworkPoliciesToNotExist(client calicoclient.ProjectcalicoV3I
 // WaitForGlobalNetworkPoliciesToExist waits for the GlobalNetworkPolicy with the given name
 // to exist.
 func WaitForGlobalNetworkPoliciesToExist(client calicoclient.ProjectcalicoV3Interface, name string) error {
-	return wait.PollImmediate(500*time.Millisecond, wait.ForeverTestTimeout,
-		func() (bool, error) {
+	return wait.PollUntilContextTimeout(context.Background(), 500*time.Millisecond, wait.ForeverTestTimeout, true,
+		func(ctx context.Context) (bool, error) {
 			klog.V(5).Infof("Waiting for serviceClass %v to exist", name)
 			_, err := client.GlobalNetworkPolicies().Get(context.Background(), name, metav1.GetOptions{})
 			if nil == err {
diff --git a/kube-controllers/pkg/controllers/flannelmigration/k8s_resources.go b/kube-controllers/pkg/controllers/flannelmigration/k8s_resources.go
index 8475b0f43..4e400ed3b 100644
--- a/kube-controllers/pkg/controllers/flannelmigration/k8s_resources.go
+++ b/kube-controllers/pkg/controllers/flannelmigration/k8s_resources.go
@@ -88,7 +88,7 @@ func (d daemonset) getContainerSpec(k8sClientset *kubernetes.Clientset, namespac
 			return &c, nil
 		}
 	}
-	return nil, fmt.Errorf("No container with name %s found in daemonset", containerName)
+	return nil, fmt.Errorf("no container with name %s found in daemonset", containerName)
 }
 
 // Get container image from a container spec.
@@ -115,13 +115,13 @@ func (d daemonset) GetContainerEnv(k8sClientset *kubernetes.Clientset, namespace
 		}
 	}
 
-	return "", fmt.Errorf("No Env with name %s found in container %s", envName, containerName)
+	return "", fmt.Errorf("no Env with name %s found in container %s", envName, containerName)
 }
 
 // Wait for daemonset to disappear.
 func (d daemonset) WaitForDaemonsetNotFound(k8sClientset *kubernetes.Clientset, namespace string, interval, timeout time.Duration) error {
-	return wait.PollImmediate(interval, timeout, func() (bool, error) {
-		_, err := k8sClientset.AppsV1().DaemonSets(namespace).Get(context.Background(), string(d), metav1.GetOptions{})
+	return wait.PollUntilContextTimeout(context.Background(), interval, timeout, true, func(ctx context.Context) (bool, error) {
+		_, err := k8sClientset.AppsV1().DaemonSets(namespace).Get(ctx, string(d), metav1.GetOptions{})
 		if apierrs.IsNotFound(err) {
 			return true, nil
 		}
@@ -288,8 +288,8 @@ func getPodContainerLog(k8sClientSet *kubernetes.Clientset, namespace, podName,
 
 // waitForPodSuccessTimeout returns nil if the pod reached state success, or an error if it reached failure or ran too long.
 func waitForPodSuccessTimeout(k8sClientset *kubernetes.Clientset, podName, namespace string, interval, timeout time.Duration) error {
-	return wait.PollImmediate(interval, timeout, func() (bool, error) {
-		pod, err := k8sClientset.CoreV1().Pods(namespace).Get(context.Background(), podName, metav1.GetOptions{})
+	return wait.PollUntilContextTimeout(context.Background(), interval, timeout, true, func(ctx context.Context) (bool, error) {
+		pod, err := k8sClientset.CoreV1().Pods(namespace).Get(ctx, podName, metav1.GetOptions{})
 		if err != nil {
 			// Cannot get pod yet, retry.
 			return false, err
@@ -312,8 +312,8 @@ type k8snode string
 // If node labels has been set already, do nothing.
 func (n k8snode) addNodeLabels(k8sClientset *kubernetes.Clientset, labelMaps ...map[string]string) error {
 	nodeName := string(n)
-	return wait.PollImmediate(1*time.Second, 1*time.Minute, func() (bool, error) {
-		node, err := k8sClientset.CoreV1().Nodes().Get(context.Background(), nodeName, metav1.GetOptions{})
+	return wait.PollUntilContextTimeout(context.Background(), 1*time.Second, 1*time.Minute, true, func(ctx context.Context) (bool, error) {
+		node, err := k8sClientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
 		if err != nil {
 			return false, err
 		}
@@ -330,7 +330,7 @@ func (n k8snode) addNodeLabels(k8sClientset *kubernetes.Clientset, labelMaps ...
 		}
 
 		if needUpdate {
-			_, err := k8sClientset.CoreV1().Nodes().Update(context.Background(), node, metav1.UpdateOptions{})
+			_, err := k8sClientset.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})
 			if err == nil {
 				return true, nil
 			}
@@ -351,8 +351,8 @@ func (n k8snode) addNodeLabels(k8sClientset *kubernetes.Clientset, labelMaps ...
 // If node labels do not exist, do nothing.
 func (n k8snode) removeNodeLabels(k8sClientset *kubernetes.Clientset, labelMaps ...map[string]string) error {
 	nodeName := string(n)
-	return wait.PollImmediate(1*time.Second, 1*time.Minute, func() (bool, error) {
-		node, err := k8sClientset.CoreV1().Nodes().Get(context.Background(), nodeName, metav1.GetOptions{})
+	return wait.PollUntilContextTimeout(context.Background(), 1*time.Second, 1*time.Minute, true, func(ctx context.Context) (bool, error) {
+		node, err := k8sClientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
 		if err != nil {
 			return false, err
 		}
@@ -368,7 +368,7 @@ func (n k8snode) removeNodeLabels(k8sClientset *kubernetes.Clientset, labelMaps
 		}
 
 		if needUpdate {
-			_, err := k8sClientset.CoreV1().Nodes().Update(context.Background(), node, metav1.UpdateOptions{})
+			_, err := k8sClientset.CoreV1().Nodes().Update(ctx, node, metav1.UpdateOptions{})
 			if err == nil {
 				return true, nil
 			}
@@ -425,7 +425,7 @@ func isPodRunningAndReady(pod *v1.Pod) bool {
 
 // Wait for a pod becoming ready on a node.
 func (n k8snode) waitPodReadyForNode(k8sClientset *kubernetes.Clientset, namespace string, interval, timeout time.Duration, label map[string]string) error {
-	return wait.PollImmediate(interval, timeout, func() (bool, error) {
+	return wait.PollUntilContextTimeout(context.Background(), interval, timeout, true, func(context.Context) (bool, error) {
 		nodeName := string(n)
 		podList, err := k8sClientset.CoreV1().Pods(namespace).List(
 			context.Background(),
@@ -446,7 +446,7 @@ func (n k8snode) waitPodReadyForNode(k8sClientset *kubernetes.Clientset, namespa
 
 		if len(podList.Items) > 1 {
 			// Multiple pods, stop waiting
-			return true, fmt.Errorf("Getting multiple pod with label %v on node %s", label, nodeName)
+			return true, fmt.Errorf("getting multiple pod with label %v on node %s", label, nodeName)
 		}
 
 		pod := podList.Items[0]
@@ -486,12 +486,12 @@ func (n k8snode) execCommandInPod(k8sClientset *kubernetes.Clientset, namespace,
 
 	if !found {
 		// Can not find pod.
-		return "", fmt.Errorf("Failed to execute command in pod. Can not find pod with label in %v on node %s", label, nodeName)
+		return "", fmt.Errorf("failed to execute command in pod. Can not find pod with label in %v on node %s", label, nodeName)
 	}
 
 	if !isPodRunningAndReady(&pod) {
 		// Pod is not running and ready.
-		return "", fmt.Errorf("Failed to execute command in pod. Pod %s is not ready.", pod.Name)
+		return "", fmt.Errorf("failed to execute command in pod. Pod %s is not ready.", pod.Name)
 	}
 
 	cmdArgs := []string{"exec", pod.Name, fmt.Sprintf("--namespace=%s", namespace), fmt.Sprintf("-c=%s", containerName), "--"}
@@ -585,8 +585,8 @@ func (n k8snode) waitForNodeLabelDisappear(k8sClientset *kubernetes.Clientset, k
 	nodeName := string(n)
 	log.Infof("Waiting for node %s label %s to disappear.", nodeName, key)
 
-	return wait.PollImmediate(interval, timeout, func() (bool, error) {
-		node, err := k8sClientset.CoreV1().Nodes().Get(context.Background(), nodeName, metav1.GetOptions{})
+	return wait.PollUntilContextTimeout(context.Background(), interval, timeout, true, func(ctx context.Context) (bool, error) {
+		node, err := k8sClientset.CoreV1().Nodes().Get(ctx, nodeName, metav1.GetOptions{})
 		if err != nil {
 			// Cannot get node, something wrong, stop waiting.
 			return true, err
diff --git a/libcalico-go/lib/backend/k8s/k8s_test.go b/libcalico-go/lib/backend/k8s/k8s_test.go
index 94c6ffa88..471e3d165 100644
--- a/libcalico-go/lib/backend/k8s/k8s_test.go
+++ b/libcalico-go/lib/backend/k8s/k8s_test.go
@@ -205,7 +205,7 @@ func (c cb) ExpectExists(updates []api.Update) {
 		log.Infof("[TEST] Expecting key: %v", update.Key)
 		matches := false
 
-		_ = wait.PollImmediate(1*time.Second, 60*time.Second, func() (bool, error) {
+		_ = wait.PollUntilContextTimeout(context.Background(), 1*time.Second, 60*time.Second, true, func(ctx context.Context) (bool, error) {
 			// Get the update.
 			c.Lock.Lock()
 			u, ok := c.State[update.Key.String()]
@@ -237,7 +237,7 @@ func (c cb) ExpectDeleted(kvps []model.KVPair) {
 		log.Infof("[TEST] Not expecting key: %v", kvp.Key)
 		exists := true
 
-		_ = wait.PollImmediate(1*time.Second, 60*time.Second, func() (bool, error) {
+		_ = wait.PollUntilContextTimeout(context.Background(), 1*time.Second, 60*time.Second, true, func(ctx context.Context) (bool, error) {
 			// Get the update.
 			c.Lock.Lock()
 			update, ok := c.State[kvp.Key.String()]
diff --git a/libcalico-go/lib/upgrade/migrator/clients/v1/k8s/k8s.go b/libcalico-go/lib/upgrade/migrator/clients/v1/k8s/k8s.go
index 4848103d9..379630b60 100644
--- a/libcalico-go/lib/upgrade/migrator/clients/v1/k8s/k8s.go
+++ b/libcalico-go/lib/upgrade/migrator/clients/v1/k8s/k8s.go
@@ -16,14 +16,11 @@ package k8s
 
 import (
 	"fmt"
-	"strings"
-	"time"
 
 	log "github.com/sirupsen/logrus"
 	"k8s.io/apimachinery/pkg/runtime"
 	"k8s.io/apimachinery/pkg/runtime/schema"
 	"k8s.io/apimachinery/pkg/runtime/serializer"
-	"k8s.io/apimachinery/pkg/util/wait"
 	"k8s.io/client-go/kubernetes"
 	"k8s.io/client-go/kubernetes/scheme"
 	_ "k8s.io/client-go/plugin/pkg/client/auth" // Import all auth providers.
@@ -49,7 +46,6 @@ type KubeClient struct {
 	nodeBgpPeerClient       resources.K8sResourceClient
 	globalBgpConfigClient   resources.K8sResourceClient
 	globalFelixConfigClient resources.K8sResourceClient
-	nodeConfigClient        resources.K8sResourceClient
 }
 
 func NewKubeClient(kc *capi.KubeConfig) (*KubeClient, error) {
@@ -109,7 +105,7 @@ func NewKubeClient(kc *capi.KubeConfig) (*KubeClient, error) {
 
 	crdClientV1, err := buildCRDClientV1(*config)
 	if err != nil {
-		return nil, fmt.Errorf("Failed to build V1 CRD client: %s", err)
+		return nil, fmt.Errorf("failed to build V1 CRD client: %s", err)
 	}
 
 	kubeClient := &KubeClient{
@@ -129,54 +125,6 @@ func (c *KubeClient) IsKDD() bool {
 	return true
 }
 
-// waitForClusterType polls until GlobalFelixConfig is ready, or until 30 seconds have passed.
-func (c *KubeClient) waitForClusterType() error {
-	return wait.PollImmediate(1*time.Second, 30*time.Second, func() (bool, error) {
-		return c.ensureClusterType()
-	})
-}
-
-// ensureClusterType ensures that the ClusterType is configured.
-func (c *KubeClient) ensureClusterType() (bool, error) {
-	k := model.GlobalConfigKey{
-		Name: "ClusterType",
-	}
-	value := "KDD"
-
-	// See if a cluster type has been set.  If so, append
-	// any existing values to it.
-	ct, err := c.Get(k)
-	if err != nil {
-		if _, ok := err.(errors.ErrorResourceDoesNotExist); !ok {
-			// Resource exists but we got another error.
-			return false, err
-		}
-		// Resource does not exist.
-	}
-	rv := ""
-	if ct != nil {
-		existingValue := ct.Value.(string)
-		if !strings.Contains(existingValue, "KDD") {
-			existingValue = fmt.Sprintf("%s,KDD", existingValue)
-		}
-		value = existingValue
-		rv = ct.Revision
-	}
-	log.WithField("value", value).Debug("Setting ClusterType")
-	_, err = c.Apply(&model.KVPair{
-		Key:      k,
-		Value:    value,
-		Revision: rv,
-	})
-	if err != nil {
-		// Don't return an error, but indicate that we need
-		// to retry.
-		log.Warnf("Failed to apply ClusterType: %s", err)
-		return false, nil
-	}
-	return true, nil
-}
-
 // buildCRDClientV1 builds a RESTClient configured to interact with Calico CustomResourceDefinitions
 func buildCRDClientV1(cfg rest.Config) (*rest.RESTClient, error) {
 	// Generate config using the base config.
-- 
2.42.2

